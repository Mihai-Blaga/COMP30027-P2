{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python382jvsc74a57bd093861e993bc647e90e5e9bd3353ecaf3d7bebc109bc4c78b5428fd668748908c",
   "display_name": "Python 3.8.2 32-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "TEST_LOCATION = \"./COMP30027_2021_Project2_datasets/recipe_test.csv\"\n",
    "TRAIN_LOCATION = \"./COMP30027_2021_Project2_datasets/recipe_train.csv\"\n",
    "\n",
    "def get_data(csv_location: str) -> pd.DataFrame:\n",
    "    # Change 9999 to np.nan when reading in the data\n",
    "    data = pd.read_csv(csv_location, header = 0) \n",
    "    return data\n",
    "\n",
    "def create_csv_output(file_name: str, result: np.ndarray):\n",
    "    output = pd.DataFrame({\"duration_label\": result})\n",
    "    output.index += 1\n",
    "    output.to_csv(file_name + \".csv\", index_label = \"id\")\n",
    "\n",
    "def get_training(train_loc: str):\n",
    "    train = get_data(train_loc)\n",
    "    X = train.iloc[:, :-1]\n",
    "    y = train.iloc[:, -1]\n",
    "    return (X, y)\n",
    "\n",
    "def preprocess_training(split = 0, rs = None):\n",
    "    (X, y) = get_training(TRAIN_LOCATION)\n",
    "    X[\"n_ingredients\"] = np.log(X[\"n_ingredients\"])\n",
    "    X[\"n_steps\"] = np.log(X[\"n_steps\"])\n",
    "    X = X.loc[:, [\"n_ingredients\", \"n_steps\"]]\n",
    "\n",
    "    if split > 0:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=rs)\n",
    "    else:\n",
    "        return (X, y)\n",
    "\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    return (X_train, X_test, y_train, y_test)\n",
    "\n",
    "output = preprocess_training(split=0.2)\n",
    "print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Loading in the datasets within the zip files based on steps in README\n",
    "import pickle\n",
    "\n",
    "BASE_LOCATION = \"./COMP30027_2021_Project2_datasets/\"\n",
    "COUNT_VEC_LOCATION = BASE_LOCATION + \"recipe_text_features_countvec/\"\n",
    "DOC2VEC50_VEC_LOCATION = BASE_LOCATION + \"recipe_text_features_doc2vec50/\"\n",
    "DOC2VEC100_VEC_LOCATION = BASE_LOCATION + \"recipe_text_features_doc2vec100/\"\n",
    "\n",
    "#countvecs are essentially a one-hot-encoding of the most popular words.\n",
    "train_name_countvec = pickle.load(open(COUNT_VEC_LOCATION + \"train_name_countvectorizer.pkl\", \"rb\"))\n",
    "train_ingr_countvec = pickle.load(open(COUNT_VEC_LOCATION + \"train_ingr_countvectorizer.pkl\", \"rb\"))\n",
    "train_steps_countvec = pickle.load(open(COUNT_VEC_LOCATION + \"train_steps_countvectorizer.pkl\", \"rb\"))\n",
    "\n",
    "#bag-of-word sparse matrices. Similar to one-hot-encoding but for all words.\n",
    "#column = word occurence, row = individual instance\n",
    "train_name_vec = scipy.sparse.load_npz(COUNT_VEC_LOCATION + 'train_name_vec.npz')\n",
    "train_ingr_vec = scipy.sparse.load_npz(COUNT_VEC_LOCATION + 'train_ingr_vec.npz')\n",
    "train_steps_vec = scipy.sparse.load_npz(COUNT_VEC_LOCATION + 'train_steps_vec.npz')\n",
    "\n",
    "test_name_vec = scipy.sparse.load_npz(COUNT_VEC_LOCATION + 'test_name_vec.npz')\n",
    "test_ingr_vec = scipy.sparse.load_npz(COUNT_VEC_LOCATION + 'test_ingr_vec.npz')\n",
    "test_steps_vec = scipy.sparse.load_npz(COUNT_VEC_LOCATION + 'test_steps_vec.npz')\n",
    "\n",
    "#Doc2Vec representation with 50 features. Think of them as 50 dimensional vectors representing the words/phrases\n",
    "test_name_vec50 = pd.read_csv(DOC2VEC50_VEC_LOCATION + \"test_name_doc2vec50.csv\", index_col = False, delimiter = ',', header=None)\n",
    "test_ingr_vec50 = pd.read_csv(DOC2VEC50_VEC_LOCATION + \"test_ingr_doc2vec50.csv\", index_col = False, delimiter = ',', header=None)\n",
    "test_steps_vec50 = pd.read_csv(DOC2VEC50_VEC_LOCATION + \"test_steps_doc2vec50.csv\", index_col = False, delimiter = ',', header=None)\n",
    "\n",
    "train_name_vec50 = pd.read_csv(DOC2VEC50_VEC_LOCATION + \"train_name_doc2vec50.csv\", index_col = False, delimiter = ',', header=None)\n",
    "train_ingr_vec50 = pd.read_csv(DOC2VEC50_VEC_LOCATION + \"train_ingr_doc2vec50.csv\", index_col = False, delimiter = ',', header=None)\n",
    "train_steps_vec50 = pd.read_csv(DOC2VEC50_VEC_LOCATION + \"train_steps_doc2vec50.csv\", index_col = False, delimiter = ',', header=None)\n",
    "\n",
    "#Doc2Vec representation with 100 features. Think of them as 100 dimensional vectors representing the words/phrases\n",
    "test_name_vec100 = pd.read_csv(DOC2VEC100_VEC_LOCATION + \"test_name_doc2vec100.csv\", index_col = False, delimiter = ',', header=None)\n",
    "test_ingr_vec100 = pd.read_csv(DOC2VEC100_VEC_LOCATION + \"test_ingr_doc2vec100.csv\", index_col = False, delimiter = ',', header=None)\n",
    "test_steps_vec100 = pd.read_csv(DOC2VEC100_VEC_LOCATION + \"test_steps_doc2vec100.csv\", index_col = False, delimiter = ',', header=None)\n",
    "\n",
    "train_name_vec100 = pd.read_csv(DOC2VEC100_VEC_LOCATION + \"train_name_doc2vec100.csv\", index_col = False, delimiter = ',', header=None)\n",
    "train_ingr_vec100 = pd.read_csv(DOC2VEC100_VEC_LOCATION + \"train_ingr_doc2vec100.csv\", index_col = False, delimiter = ',', header=None)\n",
    "train_steps_vec100 = pd.read_csv(DOC2VEC100_VEC_LOCATION + \"train_steps_doc2vec100.csv\", index_col = False, delimiter = ',', header=None)\n",
    "\"\"\"\n",
    "print(\"---countvec---\")\n",
    "print(train_name_countvec.get_feature_names())\n",
    "\"\"\"\n",
    "print(\"---vec---\")\n",
    "print(type(train_name_vec))\n",
    "print(train_name_vec[0, :]) #words corresponding to an individual instance\n",
    "#print(train_name_vec)\n",
    "\n",
    "print(\"---vec50---\")\n",
    "print(type(test_name_vec50))\n",
    "print(test_name_vec50)\n",
    "print(\"---vec100---\")\n",
    "print(type(test_name_vec100))\n",
    "print(test_name_vec100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zero-R classifier\n",
    "from sklearn import dummy\n",
    "\n",
    "(X, y) = get_training(TRAIN_LOCATION)\n",
    "zero_r_clf = dummy.DummyClassifier(strategy = \"most_frequent\")\n",
    "zero_r_clf.fit(X, y)\n",
    "\n",
    "test = get_data(TEST_LOCATION)\n",
    "output = zero_r_clf.predict(test)\n",
    "create_csv_output(\"zero-r/zero-r\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVC\n",
    "from sklearn import svm\n",
    "\n",
    "(X, y) = get_training(TRAIN_LOCATION)\n",
    "X = X.to_numpy()[:, 1:3]\n",
    "y = y.to_numpy()\n",
    "\n",
    "C = 1.0  # SVM regularization parameter\n",
    "\n",
    "models = (svm.SVC(kernel='linear', C=C),\n",
    "          svm.LinearSVC(C=C, max_iter=10000),\n",
    "          svm.SVC(kernel='rbf', gamma='auto', C=C))\n",
    "\n",
    "models = (clf.fit(X, y) for clf in models)\n",
    "\n",
    "titles = ('SVC-with-linear-kernel',\n",
    "          'LinearSVC-linear-kernel',\n",
    "          'SVC-with-RBF-kernel')\n",
    "\n",
    "X_test = get_data(TEST_LOCATION)\n",
    "X_test = X_test.to_numpy()[:, 1:3]\n",
    "\n",
    "for (title, clf) in zip(titles, models):\n",
    "    print(\"predicting \" + title)\n",
    "    output = clf.predict(X_test)\n",
    "    print(\"creating output csv\")\n",
    "    create_csv_output(\"SVC/\" + title, output) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVC with \n",
    "from sklearn import svm\n",
    "\n",
    "(X, y) = preprocess_training()\n",
    "\n",
    "temp_steps_vec50 = train_steps_vec50.copy()\n",
    "temp_steps_vec50.columns = [str(label) + \"_steps\" for label in temp_steps_vec50.columns]\n",
    "\n",
    "X = pd.concat([X, temp_steps_vec50], axis = 1, join = 'inner')\n",
    "\n",
    "C = 1.0  # SVM regularization parameter\n",
    "\n",
    "model = svm.SVC(kernel='rbf', gamma='auto', C=C)\n",
    "model = model.fit(X, y)\n",
    "title = 'SVC-with-RBF-kernel'\n",
    "  \n",
    "X_test = get_data(TEST_LOCATION)\n",
    "X_test = X_test.loc[:, [\"n_steps\", \"n_ingredients\"]]\n",
    "\n",
    "temp_steps_vec50 = test_steps_vec50.copy()\n",
    "temp_steps_vec50.columns = [str(label) + \"_steps\" for label in temp_steps_vec50.columns]\n",
    "\n",
    "X_test = pd.concat([X_test, temp_steps_vec50], axis = 1, join = 'inner')\n",
    "\n",
    "output = model.predict(X_test)\n",
    "create_csv_output(\"SVC-adv/\" + title, output) \n",
    "\"\"\"\n",
    "for (title, clf) in zip(titles, models):\n",
    "    print(\"predicting \" + title)\n",
    "    output = clf.predict(X_test)\n",
    "    print(\"creating output csv\")\n",
    "    create_csv_output(\"SVC-adv/\" + title, output) \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "(X, y) = preprocess_training()\n",
    "\n",
    "temp_steps_vec50 = train_ingr_vec50.copy()\n",
    "temp_steps_vec50.columns = [str(label) + \"_steps\" for label in temp_steps_vec50.columns]\n",
    "\n",
    "X = pd.concat([X, temp_steps_vec50], axis = 1, join = 'inner')\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "score_out = []\n",
    "layer_size = [i for i in range(25, 35)]\n",
    "for i in range(len(layer_size)):\n",
    "    print(i)\n",
    "\n",
    "    pipe = make_pipeline(StandardScaler(), MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes = (layer_size[i]), random_state=42))\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    score_out.append(pipe.score(X_test, y_test))\n",
    "\n",
    "print(score_out)\n",
    "\"\"\"\n",
    "plt.scatter(score_out, alphas)\n",
    "plt.xlabel(\"scores for neural networks\")\n",
    "plt.ylabel(\"alpha score: 10^y\")\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(layer_size, score_out)\n",
    "plt.title(\"scores using adam\")\n",
    "plt.ylabel(\"scores for neural networks\")\n",
    "plt.xlabel(\"layer_size\")\n",
    "plt.savefig(\"./graphs/adam.png\", format=\"png\")\n",
    "plt.show()"
   ]
  }
 ]
}