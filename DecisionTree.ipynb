{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0db1e82c7aa95844e45fbbbbcd794a7c1e0efcba29fed4bfa108b76bffa77ce80",
   "display_name": "Python 3.8.5 64-bit ('3.8.5': pyenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_data import get_training, get_test, get_Doc2Vec, get_data\n",
    "from processing import combine_with_vec, exclude_non_numeric\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, ShuffleSplit\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "np.random.seed(30027)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = get_training()\n",
    "train_name_vec50, train_ingr_vec50, train_steps_vec50 = get_Doc2Vec(data=\"train\", num_features=50)\n",
    "train_name_vec100, train_ingr_vec100, train_steps_vec100 = get_Doc2Vec(data=\"train\", num_features=100)"
   ]
  },
  {
   "source": [
    "### Doc2Vec preparation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Doc2Vec with 50 features and non-numeric train data\n",
    "temp_X = X.copy()\n",
    "temp_X = exclude_non_numeric(temp_X)\n",
    "temp_train_name_vec50 = train_name_vec50.copy()\n",
    "temp_train_ingr_vec50 = train_ingr_vec50.copy()\n",
    "temp_train_steps_vec50 = train_steps_vec50.copy()\n",
    "X_combined_50: pd.DataFrame = combine_with_vec(temp_X, temp_train_name_vec50, temp_train_ingr_vec50, temp_train_steps_vec50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_50, X_test_50, y_train_50, y_test_50 = train_test_split(X_combined_50, (y - 1).astype(int), test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Doc2Vec with 50 features and non-numeric train data\n",
    "temp_X = X.copy()\n",
    "temp_X = exclude_non_numeric(temp_X)\n",
    "temp_train_name_vec100 = train_name_vec100.copy()\n",
    "temp_train_ingr_vec100 = train_ingr_vec100.copy()\n",
    "temp_train_steps_vec100 = train_steps_vec100.copy()\n",
    "X_combined_100: pd.DataFrame = combine_with_vec(temp_X, temp_train_name_vec100, temp_train_ingr_vec100, temp_train_steps_vec100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_100, X_test_100, y_train_100, y_test_100 = train_test_split(X_combined_100, (y - 1).astype(int), test_size=0.2)"
   ]
  },
  {
   "source": [
    "## Decision Tree pipelines"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGBM_pipeline = make_pipeline(\n",
    "    make_column_transformer((FunctionTransformer(np.log1p), [\"n_ingredients\", \"n_steps\"]), remainder='passthrough'),\n",
    "    StandardScaler(),\n",
    "    lgb.LGBMClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_pipeline = make_pipeline(\n",
    "    make_column_transformer((FunctionTransformer(np.log1p), [\"n_ingredients\", \"n_steps\"]), remainder='passthrough'),\n",
    "    StandardScaler(),\n",
    "    xgb.XGBClassifier(use_label_encoder=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForest_pipeline = make_pipeline(\n",
    "    make_column_transformer((FunctionTransformer(np.log1p), [\"n_ingredients\", \"n_steps\"]), remainder='passthrough'),\n",
    "    StandardScaler(),\n",
    "    RandomForestClassifier()\n",
    ")"
   ]
  },
  {
   "source": [
    "## LighGBM with 50 Doc2Vec features "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "%%time\n",
    "partial_LGBM_50Doc2Vec_model = LGBM_pipeline.fit(X_train_50, y_train_50)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 41.5 s, sys: 2.31 s, total: 43.8 s\nWall time: 5.23 s\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.71875 , 0.730625, 0.721875, 0.685625, 0.7025  ])"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "cross_val_score(LGBM_pipeline, X_test_50, y_test_50, cv=ShuffleSplit(n_splits=5, test_size=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.73"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "partial_LGBM_50Doc2Vec_model.score(X_test_50, y_test_50)\n",
    "# -> 0.73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 51.2 s, sys: 3.37 s, total: 54.6 s\nWall time: 7.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "LGBM_50Doc2Vec_model = LGBM_pipeline.fit(X_combined_50, (y - 1).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(LGBM_50Doc2Vec_model, open(\"models/LightGBM-50Doc2Vec.sav\", \"wb\"))"
   ]
  },
  {
   "source": [
    "## LightGBM with 100 Doc2Vec features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 1min 24s, sys: 4.95 s, total: 1min 29s\nWall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "partial_LGBM_100Doc2Vec_model = LGBM_pipeline.fit(X_train_100, y_train_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.720625, 0.713125, 0.706875, 0.689375, 0.704375])"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "cross_val_score(LGBM_pipeline, X_test_100, y_test_100, cv=ShuffleSplit(n_splits=5, test_size=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7135"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "partial_LGBM_100Doc2Vec_model.score(X_test_100, y_test_100)\n",
    "# -> 0.7135"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 1min 36s, sys: 5.3 s, total: 1min 41s\nWall time: 13.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "LGBM_100Doc2Vec_model = LGBM_pipeline.fit(X_combined_100, (y - 1).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(LGBM_100Doc2Vec_model, open(\"models/LightGBM-100Doc2Vec.sav\", \"wb\"))"
   ]
  },
  {
   "source": [
    "## XGBoost with 50 Doc2Vec features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[18:13:53] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "CPU times: user 9min 48s, sys: 7.5 s, total: 9min 55s\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "partial_XGB_50Doc2Vec_model = XGB_pipeline.fit(X_train_50, y_train_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[18:16:08] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "/Users/mcken/.anyenv/envs/pyenv/versions/3.8.5/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "[18:16:22] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "/Users/mcken/.anyenv/envs/pyenv/versions/3.8.5/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "[18:16:35] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "/Users/mcken/.anyenv/envs/pyenv/versions/3.8.5/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "[18:16:49] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "/Users/mcken/.anyenv/envs/pyenv/versions/3.8.5/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "[18:17:03] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "/Users/mcken/.anyenv/envs/pyenv/versions/3.8.5/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.691875, 0.71375 , 0.709375, 0.6925  , 0.685625])"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "cross_val_score(XGB_pipeline, X_test_50, y_test_50, cv=ShuffleSplit(n_splits=5, test_size=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/mcken/.anyenv/envs/pyenv/versions/3.8.5/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.727625"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "partial_XGB_50Doc2Vec_model.score(X_test_50, y_test_50)\n",
    "# -> 0.727625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[18:19:08] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "XGB_50Doc2Vec_model = XGB_pipeline.fit(X_combined_50, (y-1).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(XGB_50Doc2Vec_model, open(\"models/XGBoost-50Doc2Vec.sav\", \"wb\"))"
   ]
  },
  {
   "source": [
    "## RandomForestClassifier with 50 Doc2Vec features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 46.9 s, sys: 746 ms, total: 47.7 s\nWall time: 48.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "partial_RandomForest_50Doc2Vec_model = RandomForest_pipeline.fit(X_train_50, y_train_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.655   , 0.665   , 0.66    , 0.67125 , 0.676875])"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "cross_val_score(RandomForest_pipeline, X_test_100, y_test_100, cv=ShuffleSplit(n_splits=5, test_size=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.704375"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "partial_RandomForest_50Doc2Vec_model.score(X_test_50, y_test_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 1min 2s, sys: 1.03 s, total: 1min 3s\nWall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "RandomForest_50Doc2Vec_model = RandomForest_pipeline.fit(X_combined_50, (y-1).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(RandomForest_50Doc2Vec_model, open(\"models/RandomForest-50Doc2Vec.sav\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}