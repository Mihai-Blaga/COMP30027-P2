{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0db1e82c7aa95844e45fbbbbcd794a7c1e0efcba29fed4bfa108b76bffa77ce80",
   "display_name": "Python 3.8.5 64-bit ('3.8.5')"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import scipy.stats\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "TEST_LOCATION = \"./COMP30027_2021_Project2_datasets/recipe_test.csv\"\n",
    "TRAIN_LOCATION = \"./COMP30027_2021_Project2_datasets/recipe_train.csv\"\n",
    "\n",
    "def get_data(csv_location: str) -> pd.DataFrame:\n",
    "    # Change 9999 to np.nan when reading in the data\n",
    "    data = pd.read_csv(csv_location, header = 0) \n",
    "    return data\n",
    "\n",
    "def create_csv_output(file_name: str, result: np.ndarray):\n",
    "    output = pd.DataFrame({\"duration_label\": result})\n",
    "    output.index += 1\n",
    "    output.to_csv(file_name + \".csv\", index_label = \"id\")\n",
    "\n",
    "def get_training(train_loc: str):\n",
    "    train = get_data(train_loc)\n",
    "    X = train.iloc[:, :-1]\n",
    "    y = train.iloc[:, -1].astype(\"int32\")\n",
    "    return (X, y)\n",
    "\n",
    "def preprocess_training(split = 0, rs = None):\n",
    "    (X, y) = get_training(TRAIN_LOCATION)\n",
    "    X[\"n_ingredients\"] = np.log(X[\"n_ingredients\"])\n",
    "    X[\"n_steps\"] = np.log(X[\"n_steps\"])\n",
    "    X = X.loc[:, [\"n_ingredients\", \"n_steps\"]]\n",
    "\n",
    "    if split > 0:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=rs)\n",
    "    else:\n",
    "        return (X, y)\n",
    "\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    return (X_train, X_test, y_train, y_test)\n",
    "\n",
    "def preprocess_testing():\n",
    "    X = get_data(TEST_LOCATION)\n",
    "    X[\"n_ingredients\"] = np.log(X[\"n_ingredients\"])\n",
    "    X[\"n_steps\"] = np.log(X[\"n_steps\"])\n",
    "    X = X.loc[:, [\"n_ingredients\", \"n_steps\"]]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in the datasets within the zip files based on steps in README\n",
    "import pickle\n",
    "\n",
    "BASE_LOCATION = \"./COMP30027_2021_Project2_datasets/\"\n",
    "COUNT_VEC_LOCATION = BASE_LOCATION + \"recipe_text_features_countvec/\"\n",
    "DOC2VEC50_VEC_LOCATION = BASE_LOCATION + \"recipe_text_features_doc2vec50/\"\n",
    "DOC2VEC100_VEC_LOCATION = BASE_LOCATION + \"recipe_text_features_doc2vec100/\"\n",
    "\n",
    "#countvecs are essentially a one-hot-encoding of the most popular words.\n",
    "train_name_countvec = pickle.load(open(COUNT_VEC_LOCATION + \"train_name_countvectorizer.pkl\", \"rb\"))\n",
    "train_ingr_countvec = pickle.load(open(COUNT_VEC_LOCATION + \"train_ingr_countvectorizer.pkl\", \"rb\"))\n",
    "train_steps_countvec = pickle.load(open(COUNT_VEC_LOCATION + \"train_steps_countvectorizer.pkl\", \"rb\"))\n",
    "\n",
    "#bag-of-word sparse matrices. Similar to one-hot-encoding but for all words.\n",
    "#column = word occurence, row = individual instance\n",
    "train_name_vec = scipy.sparse.load_npz(COUNT_VEC_LOCATION + 'train_name_vec.npz')\n",
    "train_ingr_vec = scipy.sparse.load_npz(COUNT_VEC_LOCATION + 'train_ingr_vec.npz')\n",
    "train_steps_vec = scipy.sparse.load_npz(COUNT_VEC_LOCATION + 'train_steps_vec.npz')\n",
    "\n",
    "test_name_vec = scipy.sparse.load_npz(COUNT_VEC_LOCATION + 'test_name_vec.npz')\n",
    "test_ingr_vec = scipy.sparse.load_npz(COUNT_VEC_LOCATION + 'test_ingr_vec.npz')\n",
    "test_steps_vec = scipy.sparse.load_npz(COUNT_VEC_LOCATION + 'test_steps_vec.npz')\n",
    "\n",
    "#Doc2Vec representation with 50 features. Think of them as 50 dimensional vectors representing the words/phrases\n",
    "test_name_vec50 = pd.read_csv(DOC2VEC50_VEC_LOCATION + \"test_name_doc2vec50.csv\", index_col = False, delimiter = ',', header=None)\n",
    "test_ingr_vec50 = pd.read_csv(DOC2VEC50_VEC_LOCATION + \"test_ingr_doc2vec50.csv\", index_col = False, delimiter = ',', header=None)\n",
    "test_steps_vec50 = pd.read_csv(DOC2VEC50_VEC_LOCATION + \"test_steps_doc2vec50.csv\", index_col = False, delimiter = ',', header=None)\n",
    "\n",
    "train_name_vec50 = pd.read_csv(DOC2VEC50_VEC_LOCATION + \"train_name_doc2vec50.csv\", index_col = False, delimiter = ',', header=None)\n",
    "train_ingr_vec50 = pd.read_csv(DOC2VEC50_VEC_LOCATION + \"train_ingr_doc2vec50.csv\", index_col = False, delimiter = ',', header=None)\n",
    "train_steps_vec50 = pd.read_csv(DOC2VEC50_VEC_LOCATION + \"train_steps_doc2vec50.csv\", index_col = False, delimiter = ',', header=None)\n",
    "\n",
    "#Doc2Vec representation with 100 features. Think of them as 100 dimensional vectors representing the words/phrases\n",
    "test_name_vec100 = pd.read_csv(DOC2VEC100_VEC_LOCATION + \"test_name_doc2vec100.csv\", index_col = False, delimiter = ',', header=None)\n",
    "test_ingr_vec100 = pd.read_csv(DOC2VEC100_VEC_LOCATION + \"test_ingr_doc2vec100.csv\", index_col = False, delimiter = ',', header=None)\n",
    "test_steps_vec100 = pd.read_csv(DOC2VEC100_VEC_LOCATION + \"test_steps_doc2vec100.csv\", index_col = False, delimiter = ',', header=None)\n",
    "\n",
    "train_name_vec100 = pd.read_csv(DOC2VEC100_VEC_LOCATION + \"train_name_doc2vec100.csv\", index_col = False, delimiter = ',', header=None)\n",
    "train_ingr_vec100 = pd.read_csv(DOC2VEC100_VEC_LOCATION + \"train_ingr_doc2vec100.csv\", index_col = False, delimiter = ',', header=None)\n",
    "train_steps_vec100 = pd.read_csv(DOC2VEC100_VEC_LOCATION + \"train_steps_doc2vec100.csv\", index_col = False, delimiter = ',', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def preproc_vec50_kbest(k_val = 50, split = 0, rs = None):\n",
    "    (X, y) = preprocess_training()\n",
    "    X_test = preprocess_testing()\n",
    "    scaler = MinMaxScaler()\n",
    "    temp_steps_vec50 = train_steps_vec50.copy()\n",
    "    temp_ingr_vec50 = train_ingr_vec50.copy()\n",
    "    temp_name_vec50 = train_name_vec50.copy()\n",
    "\n",
    "    temp_steps_vec50.columns = [str(label) + \"_steps\" for label in temp_steps_vec50.columns]\n",
    "    temp_ingr_vec50.columns = [str(label) + \"_ingrs\" for label in temp_ingr_vec50.columns]\n",
    "    temp_name_vec50.columns = [str(label) + \"_name\" for label in temp_name_vec50.columns]\n",
    "\n",
    "    X_train_50 = pd.concat([X, temp_name_vec50, temp_ingr_vec50, temp_steps_vec50], axis = 1, join = 'inner')\n",
    "\n",
    "    if split > 0:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_train_50, y, test_size = split, random_state = rs)\n",
    "        scaler.fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        chi = SelectKBest(chi2, k=k_val)\n",
    "        X_train = chi.fit_transform(X_train, y_train)\n",
    "        X_test = chi.transform(X_test)\n",
    "        \n",
    "        return (X_train, X_test, y_train, y_test)\n",
    "    else:\n",
    "        scaler.fit(X_train_50)\n",
    "        X_train_50 = scaler.transform(X_train_50)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        k_best = SelectKBest(chi2, k=k_val).fit(X_train_50, y)\n",
    "        X_train_50 = k_best.transform(X_train_50, y)\n",
    "        X_test = k_best.transform(X_test)\n",
    "        return (X_train_50, X_test, y, None)\n",
    "\n",
    "# output = preproc_vec50_kbest()\n",
    "# print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "(X, y) = get_training(TRAIN_LOCATION)\n",
    "X = X.loc[:, [\"n_steps\", \"n_ingredients\"]]\n",
    "\n",
    "temp_train_name_vec50 = train_name_vec50.copy()\n",
    "temp_train_name_vec50.columns = [str(label) + \"_name\" for label in temp_train_name_vec50.columns]\n",
    "\n",
    "temp_train_ingr_vec50 = train_ingr_vec50.copy()\n",
    "temp_train_ingr_vec50.columns = [str(label) + \"_ingrs\" for label in temp_train_ingr_vec50.columns]\n",
    "\n",
    "temp_train_steps_vec50 = train_steps_vec50.copy()\n",
    "temp_train_steps_vec50.columns = [str(label) + \"_steps\" for label in temp_train_name_vec50.columns]\n",
    "\n",
    "X_combined = pd.concat([X, temp_train_name_vec50, temp_train_ingr_vec50, temp_train_steps_vec50], axis=1, join='inner')\n",
    "\n",
    "# You need to subtract 1 from y because it needs to be 0, 1, 2 instead of 1, 2, 3\n",
    "X_50_train, X_50_test, y_50_train, y_50_test = train_test_split(X_combined, y - 1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_50_train)\n",
    "X_50_train = scaler.transform(X_50_train)\n",
    "X_50_test = scaler.transform(X_50_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = lgb.Dataset(X_50_train, label=y_50_train)\n",
    "eval_data = lgb.Dataset(X_50_test, label=y_50_test)\n",
    "\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "gbm = lgb.train(params, train_data, valid_sets=eval_data, num_boost_round=100, verbose_eval=5)\n",
    "pickle.dump(gbm, open(\"models/LightGBM-GBDT-50.sav\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = gbm.predict(X_50_test)\n",
    "\n",
    "y_pred = list()\n",
    "for x in preds:\n",
    "    y_pred.append(np.argmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_50_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = get_data(TEST_LOCATION)\n",
    "test = get_data(TEST_LOCATION)\n",
    "X_test = X_test.loc[:, [\"n_steps\", \"n_ingredients\"]]\n",
    "\n",
    "temp_test_name_vec50 = test_name_vec50.copy()\n",
    "temp_test_name_vec50.columns = [str(label) + \"_name\" for label in temp_test_name_vec50.columns]\n",
    "\n",
    "temp_test_ingr_vec50 = test_ingr_vec50.copy()\n",
    "temp_test_ingr_vec50.columns = [str(label) + \"_ingrs\" for label in temp_test_ingr_vec50.columns]\n",
    "\n",
    "temp_test_steps_vec50 = test_steps_vec50.copy()\n",
    "temp_test_steps_vec50.columns = [str(label) + \"_steps\" for label in temp_test_name_vec50.columns]\n",
    "\n",
    "X_test = pd.concat([X_test, temp_test_name_vec50, temp_test_ingr_vec50, temp_test_steps_vec50], axis=1, join='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_test)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_preds = gbm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_y_preds = list()\n",
    "for x in submission_preds:\n",
    "    submission_y_preds.append(np.argmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_output(\"DecisionTree/LightGBM-GBDT-50\", (np.array(submission_y_preds) + 1).astype(\"float32\") ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}